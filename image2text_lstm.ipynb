{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image2text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ecd61ffedf27419c84d738d644000623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee9af1e5346e4352991d379043c2b79c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e9d43138d03243ae919c973e11f9abc9",
              "IPY_MODEL_a34fb2b748d44862aea1ace3c84d335b"
            ]
          }
        },
        "ee9af1e5346e4352991d379043c2b79c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9d43138d03243ae919c973e11f9abc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dac04ea9b6bb4f719b164b19da696aa1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54d71e2daa5d4d0f97d7e89d894b7a53"
          }
        },
        "a34fb2b748d44862aea1ace3c84d335b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_278a1f11fea0457bbd9772c67734edba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100000/? [00:08&lt;00:00, 11506.90it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_937cae5f3aa8451baad48892799021ab"
          }
        },
        "dac04ea9b6bb4f719b164b19da696aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "54d71e2daa5d4d0f97d7e89d894b7a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "278a1f11fea0457bbd9772c67734edba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "937cae5f3aa8451baad48892799021ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4_xv70ow1g"
      },
      "source": [
        "!pip install catalyst==20.12 python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0d7nGG2e8XN"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomVerticalFlip, RandomHorizontalFlip, ToPILImage\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "import torchvision\n",
        "import catalyst\n",
        "import random\n",
        "from catalyst import dl, utils\n",
        "from catalyst.callbacks.scheduler import SchedulerCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR\n",
        "import Levenshtein\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "tqdm.pandas()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ0p_CuuNNs4",
        "outputId": "97a0323a-a3a4-4b89-abfa-894661d49e51"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDB2svjETFPv"
      },
      "source": [
        "!unzip /content/drive/MyDrive/image2text/train.zip -d /content/train/\n",
        "!unzip /content/drive/MyDrive/image2text/train_labels.csv.zip\n",
        "\n",
        "!unzip /content/drive/MyDrive/image2text/test.zip -d /content/test/\n",
        "!cp /content/drive/MyDrive/image2text/sample_submission.csv /content/sample_submission.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYG0CGqe8bC"
      },
      "source": [
        "TRAIN_SIZE = 0.98\n",
        "TRAIN_PATH = '/content/train_labels.csv'\n",
        "TEST_PATH = '/content/sample_submission.csv'\n",
        "TRAIN_IMAGES = '/content/train/'\n",
        "TEST_IMAGES = '/content/test/'\n",
        "submission = pd.read_csv(TEST_PATH)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o68OvFsA0Fc"
      },
      "source": [
        "df = pd.read_csv(TRAIN_PATH)\n",
        "df['len'] = df['InChI'].apply(len)\n",
        "df['bucket'] = pd.qcut(df['len'], 10)\n",
        "df_train, df_val = train_test_split(df, train_size = TRAIN_SIZE, random_state = 42, stratify = df['bucket'])\n",
        "# df_val, df_test = train_test_split(df_val_test, train_size = 0.33, stratify = df_val_test['bucket'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uI35wz4e8dj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ecd61ffedf27419c84d738d644000623",
            "ee9af1e5346e4352991d379043c2b79c",
            "e9d43138d03243ae919c973e11f9abc9",
            "a34fb2b748d44862aea1ace3c84d335b",
            "dac04ea9b6bb4f719b164b19da696aa1",
            "54d71e2daa5d4d0f97d7e89d894b7a53",
            "278a1f11fea0457bbd9772c67734edba",
            "937cae5f3aa8451baad48892799021ab"
          ]
        },
        "outputId": "f32d6d55-bb40-44e5-f960-0cb853160e32"
      },
      "source": [
        "char_dict = {'<PAD>': 0,\n",
        "             'InChI=1S/': 1,\n",
        "             '<UNK>': 2, \n",
        "             '<EOS>': 3}\n",
        "for _, row in tqdm(df_train.iloc[:100000].iterrows()):\n",
        "  for char in row['InChI']:\n",
        "    if char not in char_dict:\n",
        "      char_dict[char] = len(char_dict)\n",
        "\n",
        "indices_dict = dict(map(lambda x: (x[1], x[0]), char_dict.items()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecd61ffedf27419c84d738d644000623",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM1E1vfsJq8n"
      },
      "source": [
        "# гиперпараметры чувака который выложил ноутбук\n",
        "# weight_decay=1e-6\n",
        "# gradient_accumulation_steps=1\n",
        "# max_grad_norm=5\n",
        "# attention_dim=256\n",
        "# embed_dim=256\n",
        "# decoder_dim=512\n",
        "# dropout=0.5\n",
        "# num_layers = 1\n",
        "class Config:\n",
        "  max_len = 250\n",
        "  batch_size = 128\n",
        "  hidden_size = 300\n",
        "  cell_size = 300\n",
        "  num_layers = 2\n",
        "  emb_size = 300\n",
        "  attention_dim = 300\n",
        "  teacher_forcing = 1\n",
        "  dropout_emb = 0.3\n",
        "  dropout_hidden = 0.2\n",
        "  image_embedding = 512\n",
        "  device = torch.device('cuda:0')\n",
        "  char_dict = char_dict\n",
        "  indices_dict = indices_dict\n",
        "  vocab_size = len(char_dict)\n",
        "\n",
        "config = Config"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0krVFRJWqV6c"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "from torch.utils.data import Sampler\n",
        "import torch\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class SequenceLengthSampler(Sampler):\n",
        "\n",
        "    def __init__(self, data_source,\n",
        "                bucket_boundaries = [32, 86, 96, 104, 112, 121, 131, 142, 154, 172],\n",
        "                batch_size=128, drop_last=False):\n",
        "        self.data_source = data_source\n",
        "\n",
        "        self.ind_n_len = pickle.load(open('/content/drive/MyDrive/image2text/sampler/sampler_lens', 'rb'))\n",
        "        self.bucket_boundaries = bucket_boundaries\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "        self.boundaries = list(self.bucket_boundaries)\n",
        "        self.buckets_min = torch.tensor([np.iinfo(np.int32).min] + self.boundaries)\n",
        "        self.buckets_max = torch.tensor(self.boundaries + [np.iinfo(np.int32).max])\n",
        "        self.boundaries = torch.tensor(self.boundaries)\n",
        "\n",
        "    def shuffle_tensor(self, t):\n",
        "        return t[torch.randperm(len(t))]\n",
        "\n",
        "    #оптимизировать, а то долго  \n",
        "    def __iter__(self):\n",
        "        data_buckets = dict()\n",
        "        # where p is the id number and seq_len is the length of this id number. \n",
        "        for p, seq_len in self.ind_n_len:\n",
        "            pid = self.element_to_bucket_id(p,seq_len)\n",
        "            if pid in data_buckets.keys():\n",
        "                data_buckets[pid].append(p)\n",
        "            else:\n",
        "                data_buckets[pid] = [p]\n",
        "\n",
        "        for k in data_buckets.keys():\n",
        "\n",
        "            data_buckets[k] = torch.tensor(data_buckets[k])\n",
        "\n",
        "        iter_list = []\n",
        "        for k in data_buckets.keys():\n",
        "\n",
        "            t = self.shuffle_tensor(data_buckets[k])\n",
        "            batch = torch.split(t, self.batch_size, dim=0)\n",
        "\n",
        "            if self.drop_last and len(batch[-1]) != self.batch_size:\n",
        "                batch = batch[:-1]\n",
        "\n",
        "            iter_list += batch\n",
        "\n",
        "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
        "        # size\n",
        "        for i in iter_list: \n",
        "            yield i.numpy().tolist() # as it was stored in an array\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "    \n",
        "    def element_to_bucket_id(self, x, seq_length):\n",
        "\n",
        "        valid_buckets = (seq_length >= self.buckets_min)*(seq_length < self.buckets_max)\n",
        "        bucket_id = torch.nonzero(valid_buckets)[0].item()\n",
        "        \n",
        "        return bucket_id"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBqa1N-e80W"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.attention_dim = attention_dim\n",
        "        \n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
        "        \n",
        "        self.A = nn.Linear(attention_dim,1)\n",
        "          \n",
        "    def forward(self, features, hidden_state):\n",
        "        hidden_state = hidden_state.mean(0)\n",
        "        u_hs = self.U(features)     #(batch_size,64,attention_dim)\n",
        "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
        "        \n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,64,attemtion_dim)\n",
        "        \n",
        "        attention_scores = self.A(combined_states)         #(batch_size,64,1)\n",
        "        attention_scores = attention_scores.squeeze(2)     #(batch_size,64)\n",
        "        \n",
        "        \n",
        "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,64)\n",
        "        \n",
        "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,64,features_dim)\n",
        "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,64)\n",
        "        \n",
        "        return alpha,attention_weights\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, n_channels = 3, pretrained = True, emb_size = 512):\n",
        "    super().__init__()\n",
        "    self.emb_size = emb_size\n",
        "    if n_channels == 1:\n",
        "      resnet = torchvision.models.resnet34(pretrained=False)\n",
        "      resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias = False)\n",
        "    else:\n",
        "      resnet = torchvision.models.resnet34(pretrained=pretrained)\n",
        "\n",
        "    modules = list(resnet.children())[:-2]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    # self.conv1 = Conv2D(3, 6, 256)\n",
        "    # self.conv2 = Conv2D(6, 12, 128)\n",
        "    # self.conv3 = Conv2D(12, 24, 64)\n",
        "    # self.conv4 = Conv2D(24, 24, 32)\n",
        "    # self.fc = nn.Sequential(\n",
        "    #     nn.Linear(32768, emb_size),\n",
        "    # )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    # x = x.reshape(x.shape[0], -1)\n",
        "    # x = self.fc(x)\n",
        "    x = x.permute(0, 2, 3, 1)                           #(batch_size,8,8,512)\n",
        "    x = x.view(x.size(0), -1, x.size(-1))\n",
        "    return x\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, config, pretrained = False):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.embedding = self.get_embedding(pretrained)\n",
        "    self.dropout_emb = nn.Dropout(config.dropout_emb)\n",
        "    self.dropout_hidden = nn.Dropout(config.dropout_hidden)\n",
        "    self.rnn = nn.LSTM(input_size=config.emb_size + config.image_embedding,\n",
        "                       hidden_size=config.hidden_size,\n",
        "                       num_layers = config.num_layers,\n",
        "                       batch_first = True)\n",
        "    self.attention = Attention(config.image_embedding, config.hidden_size, config.attention_dim)\n",
        "    self.fc_score = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "  def get_embedding(self, pretrained = False):\n",
        "    if pretrained:\n",
        "      weights = np.load('/content/drive/MyDrive/image2text/models/transformer_emb_300.npy')\n",
        "      weights = torch.from_numpy(weights)\n",
        "      return nn.Embedding.from_pretrained(weights, freeze = False)\n",
        "    else:\n",
        "      return nn.Embedding(config.vocab_size, config.emb_size)\n",
        "\n",
        "  def forward(self, indices, features, hidden, cell, teacher_forcing_ratio = 0.5):\n",
        "    #input - индексы\n",
        "    input = self.embedding(indices)\n",
        "    outputs = torch.zeros(input.shape[0], config.max_len, config.vocab_size).to(config.device)\n",
        "    input = input[:, 0] #(batch_size, emb_size)\n",
        "\n",
        "    for t in range(1, config.max_len-1):\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\n",
        "\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\n",
        "      hidden = self.dropout_hidden(hidden)\n",
        "\n",
        "      output = self.fc_score(output.squeeze())\n",
        "      outputs[:, t] = output\n",
        "\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = output.max(1)[1]\n",
        "      input = (indices[:, t] if teacher_force else top1)\n",
        "      input = self.dropout_emb(self.embedding(input))\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def generate(self, features, hidden, cell):\n",
        "\n",
        "    outputs = torch.zeros(features.shape[0], config.max_len, config.vocab_size, device = config.device)\n",
        "    input = config.char_dict['InChI=1S/']*torch.ones(features.shape[0], device = config.device).long()\n",
        "    input = self.embedding(input)\n",
        "\n",
        "    for t in range(1, config.max_len-1):\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\n",
        "\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\n",
        "      hidden = self.dropout_hidden(hidden)\n",
        "\n",
        "      output = self.fc_score(output.squeeze())\n",
        "      outputs[:, t] = output\n",
        "\n",
        "      input = output.max(1)[1]\n",
        "      input = self.dropout_emb(self.embedding(input))\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class Img2Text(nn.Module):\n",
        "  def __init__(self, encoder, decoder, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.fc_h0 = nn.Linear(encoder.emb_size, decoder.hidden_size)\n",
        "    self.fc_c0 = nn.Linear(encoder.emb_size, decoder.hidden_size)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text, teacher_forcing_ratio = 0.5):\n",
        "    img_vector = self.encoder(image) #(batch, 64, 512)\n",
        "\n",
        "    hidden = self.fc_h0(img_vector.mean(1).squeeze())\n",
        "    cell = self.fc_c0(img_vector.mean(1).squeeze())\n",
        "    hidden = hidden.reshape(1, hidden.shape[0], -1)\n",
        "    hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\n",
        "    cell = cell.reshape(1, cell.shape[0], -1)\n",
        "    cell = cell.repeat(self.config.num_layers, 1, 1)\n",
        "\n",
        "    outputs = self.decoder(text, img_vector, hidden, cell, teacher_forcing_ratio)\n",
        "    return outputs\n",
        "  \n",
        "  def generate(self, image):\n",
        "    self.encoder.eval()\n",
        "    self.decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      img_vector = self.encoder(image)\n",
        "\n",
        "      hidden = self.fc_h0(img_vector.mean(1).squeeze())\n",
        "      cell = self.fc_c0(img_vector.mean(1).squeeze())\n",
        "      hidden = hidden.reshape(1, hidden.shape[0], -1)\n",
        "      hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\n",
        "      cell = cell.reshape(1, cell.shape[0], -1)\n",
        "      cell = cell.repeat(self.config.num_layers, 1, 1)\n",
        "\n",
        "      outputs = self.decoder.generate(img_vector, hidden, cell)\n",
        "    return outputs"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7g3rJGzQBf"
      },
      "source": [
        "def convert_to_indices(char_dict, string):\n",
        "  indices = [char_dict['InChI=1S/']]\n",
        "  for char in string[9:]:\n",
        "    if char not in char_dict:\n",
        "      indices.append(char_dict['<UNK>'])\n",
        "    else:\n",
        "      indices.append(char_dict[char])\n",
        "  indices += [char_dict['<EOS>']]\n",
        "  return indices\n",
        "\n",
        "def convert_to_string(indices_dict, indices):\n",
        "  string = 'InChI=1S/'\n",
        "  if isinstance(indices, torch.Tensor):\n",
        "    indices = indices.detach().cpu().numpy()\n",
        "  for indx in indices[1:]:\n",
        "    if indices_dict[indx] == '<EOS>' or indices_dict[indx] == '<PAD>':\n",
        "      break\n",
        "    else:\n",
        "      string += indices_dict[indx]\n",
        "  return string\n",
        "\n",
        "def get_path(mode, image_id):\n",
        "    if mode == 'train':\n",
        "      return f'{TRAIN_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n",
        "    elif mode == 'test':\n",
        "      return f'{TEST_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n",
        "    else:\n",
        "      raise NameError\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, mode, char_dict, transforms):\n",
        "    super().__init__()\n",
        "    self.df = df\n",
        "    self.mode = mode\n",
        "    self.char_dict = char_dict\n",
        "    self.transform = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, indx):\n",
        "    row = self.df.iloc[indx]\n",
        "    image_path = get_path(self.mode, row['image_id'])\n",
        "    indices = torch.tensor(convert_to_indices(self.char_dict, row['InChI']))\n",
        "    return {'images': self.get_image(image_path),\n",
        "            'indices': indices}\n",
        "\n",
        "  def get_image(self, image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = self.transform(image=img)['image']\n",
        "    return img\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, transforms):\n",
        "    super().__init__()\n",
        "    self.df = df\n",
        "    self.transform = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, indx):\n",
        "    row = self.df.iloc[indx]\n",
        "    image_path = get_path('test', row['image_id'])\n",
        "    return {'images': self.get_image(image_path)}\n",
        "\n",
        "  def get_image(self, image_path):\n",
        "    transform = A.Compose([A.Transpose(p=1),\n",
        "                           A.VerticalFlip(p=1)\n",
        "                          ])\n",
        "    \n",
        "    img = cv2.imread(image_path)\n",
        "    h, w, _ = img.shape\n",
        "    if h > w:\n",
        "      img = transform(image=img)['image']\n",
        "    img = self.transform(image=img)['image']\n",
        "    return img\n",
        "\n",
        "def collate_fn(batch):\n",
        "  images = torch.cat([item['images'].unsqueeze(0) for item in batch], dim = 0)\n",
        "  indices = torch.zeros(len(batch), config.max_len).long()\n",
        "  for i, item in enumerate(batch):\n",
        "    ind = item['indices'][:config.max_len]\n",
        "    indices[i][:len(ind)] = ind\n",
        "\n",
        "  return {'images': images,\n",
        "          'indices': indices}"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgcgxUju5S9b"
      },
      "source": [
        "def calculate_accuracy(logits, targets, mask):\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    return accuracy_score(targets[mask], logits[mask])\n",
        "\n",
        "def calculate_f1(logits, targets, mask):\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    return f1_score(targets[mask], logits[mask], average = 'micro')\n",
        "\n",
        "def calculate_levenstein(logits, targets, indices_dict):\n",
        "  distances = []\n",
        "  logits = torch.argmax(logits, dim = -1)\n",
        "  targets = targets.detach()\n",
        "  for indx in range(len(targets)):\n",
        "    distances.append(Levenshtein.distance(convert_to_string(indices_dict, targets[indx]), \n",
        "                                          convert_to_string(indices_dict, logits[indx])))\n",
        "  return np.array(distances).mean()\n",
        "\n",
        "def calculate_levenstein_test(indices, target_indices):\n",
        "  distances = []\n",
        "  indices = indices.detach().cpu().numpy()\n",
        "  target_indices = target_indices.cpu().numpy()\n",
        "  for i in range(len(target_indices)):\n",
        "    distances.append(Levenshtein.distance(convert_to_string(config.indices_dict, target_indices[i])[6:], \n",
        "                                          convert_to_string(config.indices_dict, indices[i])[6:]))\n",
        "  return np.array(distances).mean()\n",
        "\n",
        "def predict(test_df, model, test_loader, indices_dict):\n",
        "    test_df = test_df.copy()\n",
        "    InChI = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(test_loader):\n",
        "        images = batch[\"images\"].cuda()\n",
        "        logits = model.generate(images)\n",
        "        pred_indices = torch.argmax(logits, dim = -1)\n",
        "        for row in pred_indices:\n",
        "          InChI.append(convert_to_string(indices_dict, row))\n",
        "    test_df['InChI'] = InChI\n",
        "    return test_df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTrYpKtpIDZ"
      },
      "source": [
        "class CustomRunner(dl.Runner):\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        images = batch[\"images\"]\n",
        "        indices = batch[\"indices\"]\n",
        "\n",
        "        if self.loader_key == 'valid':\n",
        "          self.model.eval()\n",
        "          logits = self.model(images, indices[:, :-1], 0)\n",
        "        else:\n",
        "          self.model.train()\n",
        "          logits = self.model(images, indices[:, :-1], config.teacher_forcing)\n",
        "        output = logits[:, 1:].reshape(-1, logits.shape[-1])\n",
        "        trg = indices[:, 1:].reshape(-1)\n",
        "          \n",
        "        loss = criterion(output, trg)\n",
        "        levenstein = calculate_levenstein(logits, indices, config.indices_dict)\n",
        "\n",
        "        batch_metrics = {\n",
        "              \"loss\": loss,\n",
        "              \"f1_score\": calculate_f1(output, trg, torch.where(trg)[0]),\n",
        "              \"levenstein\": levenstein\n",
        "              }\n",
        "        self.batch_metrics.update(batch_metrics)\n",
        "\n",
        "        if self.is_train_loader:\n",
        "          loss.backward()\n",
        "          # nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u05fmTEwvOp"
      },
      "source": [
        "EPOCHS = 1\n",
        "lr = 1e-5\n",
        "pretrained = True\n",
        "ratio = 0.1\n",
        "n_channels = 3\n",
        "transform_train  = A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomRotate90(p=.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225],\n",
        "                        always_apply=True),\n",
        "        ToTensor()\n",
        "        ])\n",
        "transform_test = A.Compose([\n",
        "    A.Resize(256, 256),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225],\n",
        "                        always_apply=True),\n",
        "    ToTensor()\n",
        "    ])\n",
        "\n",
        "train_dataset = Dataset(df_train, 'train', char_dict, transform_train)\n",
        "val_dataset = Dataset(df_val, 'train', char_dict, transform_test)\n",
        "# test_dataset = Dataset(df_test, 'train', char_dict, transform)\n",
        "# sampler = SequenceLengthSampler(train_dataset, batch_size =config.batch_size)\n",
        "#потом сохранить эту хуйню, чтобы не ждать в будущем по 2 часа\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           config.batch_size,\n",
        "                                           num_workers = 4, \n",
        "                                           collate_fn=collate_fn,\n",
        "                                           shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, config.batch_size, num_workers = 4, collate_fn = collate_fn)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "encoder = CNN(n_channels = n_channels, pretrained = pretrained, emb_size = config.image_embedding)\n",
        "decoder = RNN(config, pretrained)\n",
        "model = Img2Text(encoder, decoder, config)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = 1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0).to(config.device)\n",
        "# scheduler = OneCycleLR(optimizer, lr, pct_start = ratio, total_steps=len(train_loader)*EPOCHS)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99987)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKA0auPdaGWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae08b809-1211-426d-f38b-e809757f1111"
      },
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/image2text/logs/checkpoints/last.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "# # criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
        "# model.train()\n",
        "# EPOCH = checkpoint['epoch']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xnohDl1frD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc63fec-5dbf-4d07-884b-df4d589dfb0b"
      },
      "source": [
        "runner = CustomRunner(device = config.device)\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion= criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders={\n",
        "        'train': train_loader,\n",
        "        'valid': val_loader,\n",
        "           },\n",
        "    logdir=\"/content/drive/MyDrive/image2text/logs\",\n",
        "    num_epochs=EPOCHS,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        "    overfit=False,\n",
        "    callbacks=[SchedulerCallback(reduced_metric = 'loss', mode = 'batch')],\n",
        "    main_metric=\"levenstein\",\n",
        "    minimize_metric=True\n",
        ")\n",
        "# <Попробовать на tpu,55 V100 1s/it, P100 1.4s/it>\n",
        "# <Попробовать с одним каналом>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 * Epoch (train): 100% 18561/18561 [5:04:40<00:00,  1.02it/s, f1_score=0.914, levenstein=9.318, loss=0.227, lr=8.954e-07, momentum=0.900]\n",
            "1/1 * Epoch (valid): 100% 379/379 [02:40<00:00,  2.36it/s, f1_score=0.631, levenstein=22.010, loss=3.303]\n",
            "[2021-03-25 12:51:24,691] \n",
            "1/1 * Epoch 1 (train): f1_score=0.9290 | levenstein=8.7121 | loss=0.1994 | lr=3.773e-06 | momentum=0.9000\n",
            "1/1 * Epoch 1 (valid): f1_score=0.6585 | levenstein=18.9016 | loss=3.0847\n",
            "Top best models:\n",
            "/content/drive/MyDrive/image2text/logs/checkpoints/train.1.pth\t18.9016\n",
            "=> Loading checkpoint /content/drive/MyDrive/image2text/logs/checkpoints/best_full.pth\n",
            "loaded state checkpoint /content/drive/MyDrive/image2text/logs/checkpoints/best_full.pth (global epoch 1, epoch 1, stage train)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0ITI3LBVtc"
      },
      "source": [
        "tqdm._instances.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMrSMm4aWRT"
      },
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/logs/checkpoints/best.pth')['model_state_dict'])\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghrRzCofvXR"
      },
      "source": [
        "pd_df = predict(df_test, model, test_loader, config.indices_dict)\n",
        "pd_df['InChI_true'] = df_test['InChI']\n",
        "pd_df = pd_df.assign(levenstein = lambda x: calculate_levenstein_test(x.InChI_true.values, x.InChI.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l24k_1i_L0DM"
      },
      "source": [
        "pd_df.levenstein.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkaHRQKMcLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4e9c1d-b6d2-43e5-e6d7-f97b2b2874bb"
      },
      "source": [
        "test_dataset = TestDataset(submission, transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size)\n",
        "pd_df = predict(submission, model, test_loader, config.indices_dict)\n",
        "pd_df.to_csv('/content/drive/MyDrive/image2text/submission.csv', index = False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12626/12626 [2:31:58<00:00,  1.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3olisffumd1"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/models/resnet34_transformer.pth')['model_state_dict'])\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfhHbbpx5LVx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}