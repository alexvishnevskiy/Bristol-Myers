{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image2text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5d255463a2c412286acdd0dcfc8e38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74dc146d2dad4ed09c8ef5ee76037800",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_787d1cf464e74f70914b3246ebf33d08",
              "IPY_MODEL_d81aaf1a48164939a77e2b9e73637f58"
            ]
          }
        },
        "74dc146d2dad4ed09c8ef5ee76037800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "787d1cf464e74f70914b3246ebf33d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6197a8a59173420194d56ac32dcdea70",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d70b45e1d90742c18992329c4c4ed9c9"
          }
        },
        "d81aaf1a48164939a77e2b9e73637f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3591194e734048debcbb060b79025af5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:05&lt;00:00, 171.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6380f93193a4ca693c249c9c9870698"
          }
        },
        "6197a8a59173420194d56ac32dcdea70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d70b45e1d90742c18992329c4c4ed9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3591194e734048debcbb060b79025af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6380f93193a4ca693c249c9c9870698": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "975a83b93f514575b7c907520fb50eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_06cda2558b504034861f502bf080e0f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c0c2a6fb38c4c6fb1704d3dac161368",
              "IPY_MODEL_e85c88235bd0476f87f2738456810858"
            ]
          }
        },
        "06cda2558b504034861f502bf080e0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c0c2a6fb38c4c6fb1704d3dac161368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1150831d63fb4d628fdee3fef67d04c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46c2ba48aab84d248bf7f4b5f28a0bb8"
          }
        },
        "e85c88235bd0476f87f2738456810858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b53a2a6cecc492c9e1a531a050d0183",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2424186/? [03:43&lt;00:00, 10866.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f85dce8122d447d8a4f64fd7a0d12c6"
          }
        },
        "1150831d63fb4d628fdee3fef67d04c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46c2ba48aab84d248bf7f4b5f28a0bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b53a2a6cecc492c9e1a531a050d0183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f85dce8122d447d8a4f64fd7a0d12c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4_xv70ow1g"
      },
      "source": [
        "!pip install catalyst==20.12 python-Levenshtein wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0d7nGG2e8XN"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomVerticalFlip, RandomHorizontalFlip, ToPILImage\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "import torchvision\n",
        "import catalyst\n",
        "import random\n",
        "import re\n",
        "import wandb\n",
        "from catalyst import dl, utils\n",
        "from catalyst.callbacks.scheduler import SchedulerCallback\n",
        "from catalyst.callbacks.checkpoint import IterationCheckpointCallback\n",
        "from catalyst.callbacks.logging import VerboseLogger\n",
        "from catalyst.contrib.callbacks.wandb_logger import WandbLogger\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR\n",
        "import Levenshtein\n",
        "import pickle\n",
        "import cv2\n",
        "from PIL import Image\n",
        "tqdm.pandas()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDB2svjETFPv"
      },
      "source": [
        "!unzip /content/drive/MyDrive/image2text/train.zip -d /content/train/\n",
        "!unzip /content/drive/MyDrive/image2text/train_labels.csv.zip\n",
        "\n",
        "!unzip /content/drive/MyDrive/image2text/test.zip -d /content/test/\n",
        "!cp /content/drive/MyDrive/image2text/sample_submission.csv /content/sample_submission.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYG0CGqe8bC"
      },
      "source": [
        "TRAIN_SIZE = 0.98\n",
        "TRAIN_PATH = '/content/train_labels.csv'\n",
        "TEST_PATH = '/content/sample_submission.csv'\n",
        "TRAIN_IMAGES = '/content/train/'\n",
        "TEST_IMAGES = '/content/test/'\n",
        "submission = pd.read_csv(TEST_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCnK_JmipOli"
      },
      "source": [
        "def split_form(form):\n",
        "    string = ''\n",
        "    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n",
        "        elem = re.match(r\"\\D+\", i).group()\n",
        "        num = i.replace(elem, \"\")\n",
        "        if num == \"\":\n",
        "            string += f\"{elem} \"\n",
        "        else:\n",
        "            string += f\"{elem} {str(num)} \"\n",
        "    return string.rstrip(' ')\n",
        "\n",
        "def split_form2(form):\n",
        "    string = ''\n",
        "    for i in re.findall(r\"[a-z][^a-z]*\", form):\n",
        "        elem = i[0]\n",
        "        num = i.replace(elem, \"\").replace('/', \"\")\n",
        "        num_string = ''\n",
        "        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n",
        "            num_list = list(re.findall(r'\\d+', j))\n",
        "            assert len(num_list) == 1, f\"len(num_list) != 1\"\n",
        "            _num = num_list[0]\n",
        "            if j == _num:\n",
        "                num_string += f\"{_num} \"\n",
        "            else:\n",
        "                extra = j.replace(_num, \"\")\n",
        "                num_string += f\"{_num} {' '.join(list(extra))} \"\n",
        "        string += f\"/{elem} {num_string}\"\n",
        "    return string.rstrip(' ')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RLPJy2YsR3G"
      },
      "source": [
        "df = pd.read_csv(TRAIN_PATH)\n",
        "df['InChI_1'] = df['InChI'].progress_apply(lambda x: x.split('/')[1])\n",
        "df['InChI_text'] = df['InChI_1'].progress_apply(split_form) + ' ' + \\\n",
        "                   df['InChI'].apply(lambda x: '/'.join(x.split('/')[2:])).progress_apply(split_form2).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c5d255463a2c412286acdd0dcfc8e38e",
            "74dc146d2dad4ed09c8ef5ee76037800",
            "787d1cf464e74f70914b3246ebf33d08",
            "d81aaf1a48164939a77e2b9e73637f58",
            "6197a8a59173420194d56ac32dcdea70",
            "d70b45e1d90742c18992329c4c4ed9c9",
            "3591194e734048debcbb060b79025af5",
            "b6380f93193a4ca693c249c9c9870698"
          ]
        },
        "id": "qztw_oiC-uYr",
        "outputId": "ac38a60b-671b-47bf-d6cc-13a7dd233691"
      },
      "source": [
        "h, w = [], []\n",
        "for i in tqdm(range(1000)):\n",
        "  path = get_path('train', df.iloc[i].image_id)\n",
        "  img = cv2.imread(path)\n",
        "  height, width, _ = img.shape\n",
        "  h.append(height)\n",
        "  w.append(width)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d255463a2c412286acdd0dcfc8e38e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uI35wz4e8dj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "975a83b93f514575b7c907520fb50eac",
            "06cda2558b504034861f502bf080e0f0",
            "4c0c2a6fb38c4c6fb1704d3dac161368",
            "e85c88235bd0476f87f2738456810858",
            "1150831d63fb4d628fdee3fef67d04c8",
            "46c2ba48aab84d248bf7f4b5f28a0bb8",
            "6b53a2a6cecc492c9e1a531a050d0183",
            "1f85dce8122d447d8a4f64fd7a0d12c6"
          ]
        },
        "outputId": "1aa8b173-840c-4818-bb73-a04705212043"
      },
      "source": [
        "char_dict = {'<PAD>': 0,\n",
        "             'InChI=1S/': 1,\n",
        "             '<UNK>': 2, \n",
        "             '<EOS>': 3}\n",
        "for _, row in tqdm(df.iterrows()):\n",
        "  for char in row['InChI_text'].split(' '):\n",
        "    if char not in char_dict:\n",
        "      char_dict[char] = len(char_dict)\n",
        "\n",
        "indices_dict = dict(map(lambda x: (x[1], x[0]), char_dict.items()))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "975a83b93f514575b7c907520fb50eac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFq86P43v0i2"
      },
      "source": [
        "df['len'] = df['InChI_text'].apply(lambda x: len(x.split(' ')))\n",
        "df['bucket'] = pd.qcut(df['len'], 10)\n",
        "#переделать все таки на muscar-scarfy\n",
        "df_train, df_val = train_test_split(df, train_size = TRAIN_SIZE, random_state = 42, stratify = df['bucket'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM1E1vfsJq8n"
      },
      "source": [
        "class Config:\n",
        "  max_len = 170\n",
        "  batch_size = 55\n",
        "  hidden_size = 512\n",
        "  cell_size = 512\n",
        "  num_layers = 1\n",
        "  emb_size = 512\n",
        "  attention_dim = 512\n",
        "  teacher_forcing = 1\n",
        "  # dropout_emb = 0.3\n",
        "  dropout_hidden = 0.5\n",
        "  image_embedding = 512\n",
        "  device = torch.device('cuda:0')\n",
        "  char_dict = char_dict\n",
        "  indices_dict = indices_dict\n",
        "  vocab_size = len(char_dict)\n",
        "\n",
        "config = Config"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0krVFRJWqV6c"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "from torch.utils.data import Sampler\n",
        "import torch\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class SequenceLengthSampler(Sampler):\n",
        "\n",
        "    def __init__(self, data_source,\n",
        "                bucket_boundaries = [32, 86, 96, 104, 112, 121, 131, 142, 154, 172],\n",
        "                batch_size=128, drop_last=False):\n",
        "        self.data_source = data_source\n",
        "\n",
        "        self.ind_n_len = pickle.load(open('/content/drive/MyDrive/image2text/sampler/sampler_lens', 'rb'))\n",
        "        self.bucket_boundaries = bucket_boundaries\n",
        "        self.batch_size = batch_size\n",
        "        self.drop_last = drop_last\n",
        "\n",
        "        self.boundaries = list(self.bucket_boundaries)\n",
        "        self.buckets_min = torch.tensor([np.iinfo(np.int32).min] + self.boundaries)\n",
        "        self.buckets_max = torch.tensor(self.boundaries + [np.iinfo(np.int32).max])\n",
        "        self.boundaries = torch.tensor(self.boundaries)\n",
        "\n",
        "    def shuffle_tensor(self, t):\n",
        "        return t[torch.randperm(len(t))]\n",
        "\n",
        "    #оптимизировать, а то долго  \n",
        "    def __iter__(self):\n",
        "        data_buckets = dict()\n",
        "        # where p is the id number and seq_len is the length of this id number. \n",
        "        for p, seq_len in self.ind_n_len:\n",
        "            pid = self.element_to_bucket_id(p,seq_len)\n",
        "            if pid in data_buckets.keys():\n",
        "                data_buckets[pid].append(p)\n",
        "            else:\n",
        "                data_buckets[pid] = [p]\n",
        "\n",
        "        for k in data_buckets.keys():\n",
        "\n",
        "            data_buckets[k] = torch.tensor(data_buckets[k])\n",
        "\n",
        "        iter_list = []\n",
        "        for k in data_buckets.keys():\n",
        "\n",
        "            t = self.shuffle_tensor(data_buckets[k])\n",
        "            batch = torch.split(t, self.batch_size, dim=0)\n",
        "\n",
        "            if self.drop_last and len(batch[-1]) != self.batch_size:\n",
        "                batch = batch[:-1]\n",
        "\n",
        "            iter_list += batch\n",
        "\n",
        "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
        "        # size\n",
        "        for i in iter_list: \n",
        "            yield i.numpy().tolist() # as it was stored in an array\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data_source)\n",
        "    \n",
        "    def element_to_bucket_id(self, x, seq_length):\n",
        "\n",
        "        valid_buckets = (seq_length >= self.buckets_min)*(seq_length < self.buckets_max)\n",
        "        bucket_id = torch.nonzero(valid_buckets)[0].item()\n",
        "        \n",
        "        return bucket_id"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBqa1N-e80W"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.attention_dim = attention_dim\n",
        "        \n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
        "        \n",
        "        self.A = nn.Linear(attention_dim,1)\n",
        "          \n",
        "    def forward(self, features, hidden_state):\n",
        "        hidden_state = hidden_state.mean(0)\n",
        "        u_hs = self.U(features)     #(batch_size,64,attention_dim)\n",
        "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
        "        \n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,64,attemtion_dim)\n",
        "        \n",
        "        attention_scores = self.A(combined_states)         #(batch_size,64,1)\n",
        "        attention_scores = attention_scores.squeeze(2)     #(batch_size,64)\n",
        "        \n",
        "        \n",
        "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,64)\n",
        "        \n",
        "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,64,features_dim)\n",
        "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,64)\n",
        "        \n",
        "        return alpha,attention_weights\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    resnet = torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "    modules = list(resnet.children())[:-2]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    # self.conv1 = Conv2D(3, 6, 256)\n",
        "    # self.conv2 = Conv2D(6, 12, 128)\n",
        "    # self.conv3 = Conv2D(12, 24, 64)\n",
        "    # self.conv4 = Conv2D(24, 24, 32)\n",
        "    # self.fc = nn.Sequential(\n",
        "    #     nn.Linear(32768, emb_size),\n",
        "    # )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.resnet(x)\n",
        "    # x = x.reshape(x.shape[0], -1)\n",
        "    # x = self.fc(x)\n",
        "    x = x.permute(0, 2, 3, 1)                           #(batch_size,8,8,512)\n",
        "    x = x.view(x.size(0), -1, x.size(-1))\n",
        "    return x\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.embedding = nn.Embedding(config.vocab_size, config.emb_size)\n",
        "    self.dropout_hidden = nn.Dropout(config.dropout_hidden)\n",
        "    self.rnn = nn.LSTM(input_size=config.emb_size + config.image_embedding,\n",
        "                       hidden_size=config.hidden_size,\n",
        "                       num_layers = config.num_layers,\n",
        "                       batch_first = True)\n",
        "    self.attention = Attention(config.image_embedding, config.hidden_size, config.attention_dim)\n",
        "    self.fc_score = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "  def forward(self, indices, features, hidden, cell, teacher_forcing_ratio = 0.5):\n",
        "    #input - индексы\n",
        "    input = self.embedding(indices)\n",
        "    outputs = torch.zeros(input.shape[0], config.max_len, config.vocab_size).to(config.device)\n",
        "    input = input[:, 0] #(batch_size, emb_size)\n",
        "\n",
        "    for t in range(1, config.max_len-1):\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\n",
        "\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\n",
        "      hidden = self.dropout_hidden(hidden)\n",
        "\n",
        "      output = self.fc_score(output.squeeze())\n",
        "      outputs[:, t] = output\n",
        "\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      top1 = output.max(1)[1]\n",
        "      input = (indices[:, t] if teacher_force else top1)\n",
        "      input = self.embedding(input)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def generate(self, features, hidden, cell):\n",
        "\n",
        "    outputs = torch.zeros(features.shape[0], config.max_len, config.vocab_size, device = config.device)\n",
        "    input = config.char_dict['InChI=1S/']*torch.ones(features.shape[0], device = config.device).long()\n",
        "    input = self.embedding(input)\n",
        "\n",
        "    for t in range(1, config.max_len-1):\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\n",
        "\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\n",
        "      hidden = self.dropout_hidden(hidden)\n",
        "\n",
        "      output = self.fc_score(output.squeeze())\n",
        "      outputs[:, t] = output\n",
        "\n",
        "      input = output.max(1)[1]\n",
        "      input = self.embedding(input)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class Img2Text(nn.Module):\n",
        "  def __init__(self, encoder, decoder, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.fc_h0 = nn.Linear(config.image_embedding, decoder.hidden_size)\n",
        "    self.fc_c0 = nn.Linear(config.image_embedding, decoder.hidden_size)\n",
        "\n",
        "    # checkpoint = torch.load(open('/content/drive/MyDrive/image2text/models/encoder0.pkl', 'rb'))\n",
        "    # encoder.load_state_dict(checkpoint)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, image, text, teacher_forcing_ratio = 0.5):\n",
        "    img_vector = self.encoder(image) #(batch, 64, 512)\n",
        "\n",
        "    hidden = self.fc_h0(img_vector.mean(1).squeeze())\n",
        "    cell = self.fc_c0(img_vector.mean(1).squeeze())\n",
        "    hidden = hidden.reshape(1, hidden.shape[0], -1)\n",
        "    hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\n",
        "    cell = cell.reshape(1, cell.shape[0], -1)\n",
        "    cell = cell.repeat(self.config.num_layers, 1, 1)\n",
        "\n",
        "    outputs = self.decoder(text, img_vector, hidden, cell, teacher_forcing_ratio)\n",
        "    return outputs\n",
        "  \n",
        "  def generate(self, image):\n",
        "    self.encoder.eval()\n",
        "    self.decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      img_vector = self.encoder(image)\n",
        "\n",
        "      hidden = self.fc_h0(img_vector.mean(1).squeeze())\n",
        "      cell = self.fc_c0(img_vector.mean(1).squeeze())\n",
        "      hidden = hidden.reshape(1, hidden.shape[0], -1)\n",
        "      hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\n",
        "      cell = cell.reshape(1, cell.shape[0], -1)\n",
        "      cell = cell.repeat(self.config.num_layers, 1, 1)\n",
        "\n",
        "      outputs = self.decoder.generate(img_vector, hidden, cell)\n",
        "    return outputs"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7g3rJGzQBf"
      },
      "source": [
        "def convert_to_indices(char_dict, string):\n",
        "  indices = [char_dict['InChI=1S/']]\n",
        "  for char in string[9:]:\n",
        "    if char not in char_dict:\n",
        "      indices.append(char_dict['<UNK>'])\n",
        "    else:\n",
        "      indices.append(char_dict[char])\n",
        "  indices += [char_dict['<EOS>']]\n",
        "  return indices\n",
        "\n",
        "def convert_to_string(indices_dict, indices):\n",
        "  string = 'InChI=1S/'\n",
        "  if isinstance(indices, torch.Tensor):\n",
        "    indices = indices.detach().cpu().numpy()\n",
        "  for indx in indices[1:]:\n",
        "    if indices_dict[indx] == '<EOS>' or indices_dict[indx] == '<PAD>':\n",
        "      break\n",
        "    else:\n",
        "      string += indices_dict[indx]\n",
        "  return string\n",
        "\n",
        "def get_path(mode, image_id):\n",
        "    if mode == 'train':\n",
        "      return f'{TRAIN_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n",
        "    elif mode == 'test':\n",
        "      return f'{TEST_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\n",
        "    else:\n",
        "      raise NameError\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, mode, char_dict, transforms):\n",
        "    super().__init__()\n",
        "    self.df = df\n",
        "    self.mode = mode\n",
        "    self.char_dict = char_dict\n",
        "    self.transform = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, indx):\n",
        "    row = self.df.iloc[indx]\n",
        "    image_path = get_path(self.mode, row['image_id'])\n",
        "    indices = torch.tensor(convert_to_indices(self.char_dict, row['InChI']))\n",
        "    return {'images': self.get_image(image_path),\n",
        "            'indices': indices}\n",
        "\n",
        "  def get_image(self, image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = self.transform(image=img)['image']\n",
        "    return img\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, df, transforms):\n",
        "    super().__init__()\n",
        "    self.df = df\n",
        "    self.transform = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, indx):\n",
        "    row = self.df.iloc[indx]\n",
        "    image_path = get_path('test', row['image_id'])\n",
        "    return {'images': self.get_image(image_path)}\n",
        "\n",
        "  def get_image(self, image_path):\n",
        "    transform = A.Compose([A.Transpose(p=1),\n",
        "                           A.VerticalFlip(p=1)\n",
        "                          ])\n",
        "    \n",
        "    img = cv2.imread(image_path)\n",
        "    h, w, _ = img.shape\n",
        "    if h > w:\n",
        "      img = transform(image=img)['image']\n",
        "    img = self.transform(image=img)['image']\n",
        "    return img\n",
        "\n",
        "def collate_fn(batch):\n",
        "  images = torch.cat([item['images'].unsqueeze(0) for item in batch], dim = 0)\n",
        "  indices = torch.zeros(len(batch), config.max_len).long()\n",
        "  for i, item in enumerate(batch):\n",
        "    ind = item['indices'][:config.max_len]\n",
        "    indices[i][:len(ind)] = ind\n",
        "\n",
        "  return {'images': images,\n",
        "          'indices': indices}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgcgxUju5S9b"
      },
      "source": [
        "def calculate_accuracy(logits, targets, mask):\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    return accuracy_score(targets[mask], logits[mask])\n",
        "\n",
        "def calculate_f1(logits, targets, mask):\n",
        "    mask = mask.detach().cpu().numpy()\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    return f1_score(targets[mask], logits[mask], average = 'micro')\n",
        "\n",
        "def calculate_levenstein(logits, targets, indices_dict):\n",
        "  distances = []\n",
        "  logits = torch.argmax(logits, dim = -1)\n",
        "  targets = targets.detach()\n",
        "  for indx in range(len(targets)):\n",
        "    distances.append(Levenshtein.distance(convert_to_string(indices_dict, targets[indx]), \n",
        "                                          convert_to_string(indices_dict, logits[indx])))\n",
        "  return np.array(distances).mean()\n",
        "\n",
        "def calculate_levenstein_test(indices, target_indices):\n",
        "  distances = []\n",
        "  indices = indices.detach().cpu().numpy()\n",
        "  target_indices = target_indices.cpu().numpy()\n",
        "  for i in range(len(target_indices)):\n",
        "    distances.append(Levenshtein.distance(convert_to_string(config.indices_dict, target_indices[i])[6:], \n",
        "                                          convert_to_string(config.indices_dict, indices[i])[6:]))\n",
        "  return np.array(distances).mean()\n",
        "\n",
        "def predict(test_df, model, test_loader, indices_dict):\n",
        "    test_df = test_df.copy()\n",
        "    InChI = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(test_loader):\n",
        "        images = batch[\"images\"].cuda()\n",
        "        logits = model.generate(images)\n",
        "        pred_indices = torch.argmax(logits, dim = -1)\n",
        "        for row in pred_indices:\n",
        "          InChI.append(convert_to_string(indices_dict, row))\n",
        "    test_df['InChI'] = InChI\n",
        "    return test_df"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTrYpKtpIDZ"
      },
      "source": [
        "class CustomRunner(dl.Runner):\n",
        "\n",
        "    def _handle_batch(self, batch):\n",
        "        images = batch[\"images\"]\n",
        "        indices = batch[\"indices\"]\n",
        "\n",
        "        if self.loader_key == 'valid':\n",
        "          self.model.eval()\n",
        "          logits = self.model(images, indices[:, :-1], 0)\n",
        "        else:\n",
        "          self.model.train()\n",
        "          logits = self.model(images, indices[:, :-1], config.teacher_forcing)\n",
        "        output = logits[:, 1:].reshape(-1, logits.shape[-1])\n",
        "        trg = indices[:, 1:].reshape(-1)\n",
        "          \n",
        "        loss = criterion(output, trg)\n",
        "        levenstein = calculate_levenstein(logits, indices, config.indices_dict)\n",
        "\n",
        "        batch_metrics = {\n",
        "              \"loss\": loss,\n",
        "              \"f1_score\": calculate_f1(output, trg, torch.where(trg)[0]),\n",
        "              \"levenstein\": levenstein\n",
        "              }\n",
        "        self.batch_metrics.update(batch_metrics)\n",
        "\n",
        "        if self.is_train_loader:\n",
        "          loss.backward()\n",
        "          # nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
        "          self.optimizer.step()\n",
        "          self.optimizer.zero_grad()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u05fmTEwvOp"
      },
      "source": [
        "EPOCHS = 3\n",
        "height = 400\n",
        "width = 400\n",
        "lr_encoder = 1e-5\n",
        "lr_decoder = 1e-4\n",
        "ratio = 0.1\n",
        "transform_train  = A.Compose([\n",
        "        A.Resize(height, width),\n",
        "        A.RandomRotate90(p=.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225],\n",
        "                        always_apply=True),\n",
        "        ToTensor()\n",
        "        ])\n",
        "transform_test = A.Compose([\n",
        "    A.Resize(height, width),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225],\n",
        "                        always_apply=True),\n",
        "    ToTensor()\n",
        "    ])\n",
        "\n",
        "train_dataset = Dataset(df_train, 'train', char_dict, transform_train)\n",
        "val_dataset = Dataset(df_val, 'train', char_dict, transform_test)\n",
        "# test_dataset = Dataset(df_test, 'train', char_dict, transform)\n",
        "# sampler = SequenceLengthSampler(train_dataset, batch_size =config.batch_size)\n",
        "#потом сохранить эту хуйню, чтобы не ждать в будущем по 2 часа\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           config.batch_size,\n",
        "                                           num_workers = 4, \n",
        "                                           collate_fn=collate_fn,\n",
        "                                           shuffle = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, config.batch_size, num_workers = 4, collate_fn = collate_fn)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size, collate_fn=collate_fn)\n",
        "\n",
        "encoder = CNN()\n",
        "decoder = RNN(config)\n",
        "model = Img2Text(encoder, decoder, config)\n",
        "# optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr = lr_encoder)\n",
        "# optimizer_decoder = torch.optim.Adam(model.decoder.parameters(), lr = lr_decoder)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, weight_decay= 1e-4) #add weight decay or AdamW\n",
        "scheduler = OneCycleLR(optimizer, 1e-5, pct_start = ratio, total_steps=len(train_loader)*EPOCHS)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5, eta_min=1e-7)\n",
        "# scheduler_encoder = OneCycleLR(optimizer_encoder, lr_encoder, pct_start = ratio, total_steps=len(train_loader)*EPOCHS)\n",
        "# scheduler_decoder = OneCycleLR(optimizer_decoder, lr_decoder, pct_start = ratio, total_steps=len(train_loader)*EPOCHS)\n",
        "# scheduler_encoder = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_encoder, EPOCHS) T_max=4, lr_min = 1e-7\n",
        "# scheduler_decoder = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_decoder, EPOCHS)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99987)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKA0auPdaGWS"
      },
      "source": [
        "checkpoint = torch.load(r\"/content/drive/MyDrive/image2text/models/resnet_34_400.pth\", map_location=config.device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "# criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
        "model.train()\n",
        "# EPOCH = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTbY22q4DWB6"
      },
      "source": [
        "for p in model.encoder.parameters():\n",
        "  p.requires_grad = False"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbYP0cRR227X"
      },
      "source": [
        "wandb.init(project='brystol-myers', entity='sashnevskiy', reinit = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xnohDl1frD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b9ffb6-4bec-4110-ae0c-b1f7af95c645"
      },
      "source": [
        "runner = CustomRunner(device = config.device)\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion= criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders={\n",
        "        'train': train_loader,\n",
        "        'valid': val_loader,\n",
        "    },\n",
        "    logdir=\"/content/drive/MyDrive/image2text/logs\",\n",
        "    num_epochs=EPOCHS,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        "    overfit=False,\n",
        "    callbacks=[SchedulerCallback(reduced_metric = 'loss', mode = 'batch'),\n",
        "              #  WandbLogger(project=\"brystol-myers\",\n",
        "                           \n",
        "              #              log_on_batch_end = True,\n",
        "              #              name = \"resnet34_400\",\n",
        "              #              config = {'lr/encoder': lr_encoder,\n",
        "              #                        'lr/decoder': lr_decoder,\n",
        "              #                        'max_len': config.max_len,\n",
        "              #                        'batch_size': config.batch_size,\n",
        "              #                        'hidden_size': config.hidden_size,\n",
        "              #                        'num_layers': config.num_layers,\n",
        "              #                        'emb_size': config.emb_size,\n",
        "              #                        'attention_dim': config.attention_dim,\n",
        "              #                        'dropout_hidden': config.dropout_hidden,\n",
        "              #                        'image_embedding': config.image_embedding}),\n",
        "               IterationCheckpointCallback(period = 15000)],\n",
        "    main_metric=\"levenstein\",\n",
        "    minimize_metric=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/3 * Epoch (train):  35% 15000/43195 [2:21:58<4:28:03,  1.75it/s, f1_score=0.968, levenstein=7.764, loss=0.084, lr=9.992e-06, momentum=0.850]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.1.iter.15000.pth\n",
            "1/3 * Epoch (train):  69% 30000/43195 [4:43:59<2:04:30,  1.77it/s, f1_score=0.944, levenstein=7.255, loss=0.158, lr=9.482e-06, momentum=0.855]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.1.iter.30000.pth\n",
            "1/3 * Epoch (train): 100% 43195/43195 [6:49:09<00:00,  1.76it/s, f1_score=0.951, levenstein=5.844, loss=0.140, lr=8.431e-06, momentum=0.866]\n",
            "1/3 * Epoch (valid): 100% 882/882 [05:14<00:00,  2.80it/s, f1_score=0.808, levenstein=10.828, loss=2.637]\n",
            "[2021-04-04 18:39:51,762] \n",
            "1/3 * Epoch 1 (train): f1_score=0.9595 | levenstein=5.4009 | loss=0.1131 | lr=8.186e-06 | momentum=0.8687\n",
            "1/3 * Epoch 1 (valid): f1_score=0.7684 | levenstein=12.9021 | loss=3.2207\n",
            "2/3 * Epoch (train):   2% 923/43195 [08:45<6:38:23,  1.77it/s, f1_score=0.966, levenstein=3.764, loss=0.097, lr=8.340e-06, momentum=0.867]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.2.iter.45000.pth\n",
            "2/3 * Epoch (train):  37% 15923/43195 [2:33:56<4:26:10,  1.71it/s, f1_score=0.938, levenstein=7.982, loss=0.170, lr=6.608e-06, momentum=0.884]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.2.iter.60000.pth\n",
            "2/3 * Epoch (train):  72% 30923/43195 [4:59:47<1:59:35,  1.71it/s, f1_score=0.944, levenstein=7.255, loss=0.162, lr=4.617e-06, momentum=0.904]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.2.iter.75000.pth\n",
            "2/3 * Epoch (train): 100% 43195/43195 [6:59:10<00:00,  1.72it/s, f1_score=0.953, levenstein=5.531, loss=0.148, lr=3.020e-06, momentum=0.920]\n",
            "2/3 * Epoch (valid): 100% 882/882 [05:19<00:00,  2.76it/s, f1_score=0.755, levenstein=12.138, loss=3.168]\n",
            "[2021-04-05 01:44:45,874] \n",
            "2/3 * Epoch 2 (train): f1_score=0.9523 | levenstein=6.2613 | loss=0.1381 | lr=5.820e-06 | momentum=0.8918\n",
            "2/3 * Epoch 2 (valid): f1_score=0.7529 | levenstein=13.6498 | loss=3.1621\n",
            "3/3 * Epoch (train):   4% 1846/43195 [17:54<6:36:01,  1.74it/s, f1_score=0.953, levenstein=5.436, loss=0.136, lr=2.794e-06, momentum=0.922]\n",
            "Saved checkpoint at /content/drive/MyDrive/image2text/logs/checkpoints/train.epoch.3.iter.90000.pth\n",
            "3/3 * Epoch (train):  33% 14107/43195 [2:16:23<4:45:43,  1.70it/s, f1_score=0.950, levenstein=6.182, loss=0.145, lr=1.458e-06, momentum=0.935]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0ITI3LBVtc"
      },
      "source": [
        "tqdm._instances.clear()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMrSMm4aWRT"
      },
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/logs/checkpoints/best.pth')['model_state_dict'])\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghrRzCofvXR"
      },
      "source": [
        "pd_df = predict(df_test, model, test_loader, config.indices_dict)\n",
        "pd_df['InChI_true'] = df_test['InChI']\n",
        "pd_df = pd_df.assign(levenstein = lambda x: calculate_levenstein_test(x.InChI_true.values, x.InChI.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l24k_1i_L0DM"
      },
      "source": [
        "pd_df.levenstein.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkaHRQKMcLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4e9c1d-b6d2-43e5-e6d7-f97b2b2874bb"
      },
      "source": [
        "test_dataset = TestDataset(submission, transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size)\n",
        "pd_df = predict(submission, model, test_loader, config.indices_dict)\n",
        "pd_df.to_csv('/content/drive/MyDrive/image2text/submission.csv', index = False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12626/12626 [2:31:58<00:00,  1.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3olisffumd1"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/models/resnet34_transformer.pth')['model_state_dict'])\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfhHbbpx5LVx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}