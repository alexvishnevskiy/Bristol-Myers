{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image2text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4_xv70ow1g"
      },
      "source": [
        "!pip install catalyst==20.12 python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0d7nGG2e8XN"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomVerticalFlip, RandomHorizontalFlip, ToPILImage\r\n",
        "import torchvision\r\n",
        "import catalyst\r\n",
        "import random\r\n",
        "from catalyst import dl, utils\r\n",
        "from catalyst.callbacks.scheduler import SchedulerCallback\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score, f1_score\r\n",
        "import Levenshtein\r\n",
        "import cv2\r\n",
        "from PIL import Image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7itVi2LmIVKE"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDB2svjETFPv"
      },
      "source": [
        "!unzip /content/drive/MyDrive/image2text/train.zip -d /content/train/\r\n",
        "!unzip /content/drive/MyDrive/image2text/train_labels.csv.zip\r\n",
        "\r\n",
        "!unzip /content/drive/MyDrive/image2text/test.zip -d /content/test/\r\n",
        "!cp /content/drive/MyDrive/image2text/sample_submission.csv /content/sample_submission.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYG0CGqe8bC"
      },
      "source": [
        "TRAIN_SIZE = 0.98\r\n",
        "TRAIN_PATH = '/content/train_labels.csv'\r\n",
        "TEST_PATH = '/content/sample_submission.csv'\r\n",
        "TRAIN_IMAGES = '/content/train'\r\n",
        "TEST_IMAGES = '/content/test'\r\n",
        "submission = pd.read_csv(TEST_PATH)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o68OvFsA0Fc"
      },
      "source": [
        "df = pd.read_csv(TRAIN_PATH)\r\n",
        "df['len'] = df['InChI'].apply(len)\r\n",
        "df['bucket'] = pd.qcut(df['len'], 10)\r\n",
        "df_train, df_val = train_test_split(df, train_size = TRAIN_SIZE, random_state = 42, stratify = df['bucket'])\r\n",
        "# df_val, df_test = train_test_split(df_val_test, train_size = 0.33, stratify = df_val_test['bucket'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uI35wz4e8dj",
        "outputId": "91ebed1e-6514-4343-f8e0-e07eceb00c13"
      },
      "source": [
        "char_dict = {'<PAD>': 0,\r\n",
        "             'InChI=1S/': 1,\r\n",
        "             '<UNK>': 2, \r\n",
        "             '<EOS>': 3}\r\n",
        "for _, row in tqdm(df_train.iterrows()):\r\n",
        "  for char in row['InChI']:\r\n",
        "    if char not in char_dict:\r\n",
        "      char_dict[char] = len(char_dict)\r\n",
        "\r\n",
        "indices_dict = dict(map(lambda x: (x[1], x[0]), char_dict.items()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2375702it [04:37, 8552.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM1E1vfsJq8n"
      },
      "source": [
        "class Config:\r\n",
        "  max_len = 250\r\n",
        "  batch_size = 128\r\n",
        "  emb_size = 300\r\n",
        "  n_heads = 6\r\n",
        "  n_layers = 4\r\n",
        "  dropout_emb = 0.3\r\n",
        "  image_embedding = 512\r\n",
        "  dim_feedforward = 1024\r\n",
        "  char_dict = char_dict\r\n",
        "  indices_dict = indices_dict\r\n",
        "  vocab_size = len(char_dict)\r\n",
        "\r\n",
        "config = Config"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBqa1N-e80W"
      },
      "source": [
        "def generate_square_subsequent_mask(sz):\r\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n",
        "  return mask\r\n",
        "\r\n",
        "class CNN(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    resnet = torchvision.models.resnet34(pretrained=True)\r\n",
        "    modules = list(resnet.children())[:-2]\r\n",
        "    self.resnet = nn.Sequential(*modules)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.resnet(x)\r\n",
        "    x = x.permute(0, 2, 3, 1) #(batch_size,8,8,512)\r\n",
        "    x = x.view(x.size(0), -1, x.size(-1)) #(batch_size, 64, 512)\r\n",
        "    return x \r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "  def __init__(self, config):\r\n",
        "    super().__init__()\r\n",
        "    self.trg_emb = nn.Embedding(config.vocab_size, config.emb_size)\r\n",
        "    # weight = np.load('/content/drive/MyDrive/image2text/models/transformer_emb_300.npy')\r\n",
        "    # weight = torch.from_numpy(weight)\r\n",
        "    # self.trg_emb = nn.Embedding.from_pretrained(weight)\r\n",
        "    self.trg_pos_emb = nn.Embedding(config.max_len, config.emb_size)\r\n",
        "    self.dropout_emb = nn.Dropout(config.dropout_emb)\r\n",
        "    self.decoder_trs = self.initialize_decoder(config.emb_size, config.n_heads, config.n_layers, config.dim_feedforward)\r\n",
        "    self.fc_logits = nn.Linear(config.emb_size, config.vocab_size)\r\n",
        "\r\n",
        "  def forward(self, tgt, src):\r\n",
        "    #tgt (batch, tgt_len, emb)\r\n",
        "    #src (batch, src_len, emb)\r\n",
        "    B, trg_seq_len = tgt.shape \r\n",
        "    trg_positions = (torch.arange(0, trg_seq_len).expand(B, trg_seq_len).cuda())\r\n",
        "    trg_mask = generate_square_subsequent_mask(trg_seq_len).cuda()\r\n",
        "\r\n",
        "    embed_trg = self.trg_emb(tgt) + self.trg_pos_emb(trg_positions)\r\n",
        "    tgt_padding_mask = tgt == 0\r\n",
        "    \r\n",
        "    output = self.decoder_trs(\r\n",
        "            embed_trg.permute(1,0,2),\r\n",
        "            src.permute(1,0,2),   \r\n",
        "            tgt_mask=trg_mask, \r\n",
        "            tgt_key_padding_mask = tgt_padding_mask\r\n",
        "        ).permute(1,0,2) \r\n",
        "    logits = self.fc_logits(output) #(batch, 250, vocab)\r\n",
        "    return logits\r\n",
        "\r\n",
        "  def generate(self, images):\r\n",
        "    self.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      images = images.cuda()\r\n",
        "      B = images.shape[0]\r\n",
        "      sos = torch.tensor([config.char_dict['InChI=1S/']], dtype=torch.long).expand(B, 1).cuda()\r\n",
        "      input = sos\r\n",
        "      for _ in range(config.max_len-1):\r\n",
        "          preds = self(input, images)\r\n",
        "          preds = torch.argmax(preds, axis=-1)\r\n",
        "          input = torch.cat([sos, preds], 1)\r\n",
        "      return preds\r\n",
        "\r\n",
        "  def initialize_decoder(self, d_model, n_heads, n_layers, dim_feedforward):\r\n",
        "    decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=dim_feedforward)\r\n",
        "    transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\r\n",
        "    return transformer_decoder\r\n",
        "\r\n",
        "\r\n",
        "class Img2Text(nn.Module):\r\n",
        "  def __init__(self, config):\r\n",
        "    super().__init__()\r\n",
        "    self.config = config\r\n",
        "    self.fc_emb = nn.Linear(config.image_embedding, config.emb_size)\r\n",
        "\r\n",
        "    self.encoder = CNN()\r\n",
        "    self.decoder = Decoder(config)\r\n",
        "\r\n",
        "  def forward(self, text, image): \r\n",
        "    img_vector = self.encoder(image) #(64, batch, 512)\r\n",
        "    img_vector = self.fc_emb(img_vector) #(64, batch, emb_size)\r\n",
        "\r\n",
        "    outputs = self.decoder(text, img_vector) #(batch, seq_len, vocab)\r\n",
        "    return outputs\r\n",
        "\r\n",
        "  def generate(self, images):\r\n",
        "    img_vector = self.encoder(images)\r\n",
        "    img_vector = self.fc_emb(img_vector)\r\n",
        "    return self.decoder.generate(img_vector)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7g3rJGzQBf"
      },
      "source": [
        "def convert_to_indices(char_dict, string):\r\n",
        "  indices = [char_dict['InChI=1S/']]\r\n",
        "  for char in string[6:]:\r\n",
        "    if char not in char_dict:\r\n",
        "      indices.append(char_dict['<UNK>'])\r\n",
        "    else:\r\n",
        "      indices.append(char_dict[char])\r\n",
        "  indices += [char_dict['<EOS>']]\r\n",
        "  return indices\r\n",
        "\r\n",
        "def convert_to_string(indices_dict, indices):\r\n",
        "  string = 'InChI='\r\n",
        "  if isinstance(indices, torch.Tensor):\r\n",
        "    indices = indices.detach().cpu().numpy()\r\n",
        "  for indx in indices[1:]:\r\n",
        "    if indices_dict[indx] == '<EOS>' or indices_dict[indx] == '<PAD>':\r\n",
        "      break\r\n",
        "    else:\r\n",
        "      string += indices_dict[indx]\r\n",
        "  return string\r\n",
        "\r\n",
        "def get_path(mode, image_id):\r\n",
        "    if mode == 'train':\r\n",
        "      return f'{TRAIN_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\r\n",
        "    elif mode == 'test':\r\n",
        "      return f'{TEST_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\r\n",
        "    else:\r\n",
        "      raise NameError\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, df, mode, char_dict, transforms):\r\n",
        "    super().__init__()\r\n",
        "    self.df = df\r\n",
        "    self.mode = mode\r\n",
        "    self.char_dict = char_dict\r\n",
        "    self.transform = transforms\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.df)\r\n",
        "\r\n",
        "  def __getitem__(self, indx):\r\n",
        "    row = self.df.iloc[indx]\r\n",
        "    image_path = get_path(self.mode, row['image_id'])\r\n",
        "    indices = torch.tensor(convert_to_indices(self.char_dict, row['InChI']))\r\n",
        "    return {'images': self.get_image(image_path),\r\n",
        "            'indices': indices}\r\n",
        "\r\n",
        "  def get_image(self, image_path):\r\n",
        "    img = cv2.imread(image_path)\r\n",
        "    img = self.transform(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "class TestDataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, df, transforms):\r\n",
        "    super().__init__()\r\n",
        "    self.df = df\r\n",
        "    self.transform = transforms\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.df)\r\n",
        "\r\n",
        "  def __getitem__(self, indx):\r\n",
        "    row = self.df.iloc[indx]\r\n",
        "    image_path = get_path('test', row['image_id'])\r\n",
        "    return {'images': self.get_image(image_path)}\r\n",
        "\r\n",
        "  def get_image(self, image_path):\r\n",
        "    transform = A.Compose([A.Transpose(p=1),\r\n",
        "                           A.VerticalFlip(p=1)\r\n",
        "                          ])\r\n",
        "    \r\n",
        "    img = cv2.imread(image_path)\r\n",
        "    h, w, _ = img.shape\r\n",
        "    if h > w:\r\n",
        "      img = transform(image=img)['image']\r\n",
        "    img = self.transform(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "def collate_fn(batch):\r\n",
        "  images = torch.cat([item['images'].unsqueeze(0) for item in batch], dim = 0)\r\n",
        "  indices = torch.zeros(len(batch), config.max_len).long()\r\n",
        "  for i, item in enumerate(batch):\r\n",
        "    ind = item['indices'][:config.max_len]\r\n",
        "    indices[i][:len(ind)] = ind\r\n",
        "\r\n",
        "  return {'images': images,\r\n",
        "          'indices': indices}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgcgxUju5S9b"
      },
      "source": [
        "def calculate_accuracy(logits, targets, mask):\r\n",
        "    mask = mask.detach().cpu().numpy()\r\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\r\n",
        "    targets = targets.detach().cpu().numpy()\r\n",
        "    return accuracy_score(targets[mask], logits[mask])\r\n",
        "\r\n",
        "def calculate_f1(logits, targets, mask):\r\n",
        "    mask = mask.detach().cpu().numpy()\r\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\r\n",
        "    targets = targets.detach().cpu().numpy()\r\n",
        "    return f1_score(targets[mask], logits[mask], average = 'micro')\r\n",
        "\r\n",
        "def calculate_levenstein(logits, targets, indices_dict):\r\n",
        "  distances = []\r\n",
        "  logits = torch.argmax(logits, dim = -1)\r\n",
        "  targets = targets.detach()\r\n",
        "  for indx in range(len(targets)):\r\n",
        "    distances.append(Levenshtein.distance(convert_to_string(indices_dict, targets[indx]), \r\n",
        "                                          convert_to_string(indices_dict, logits[indx])))\r\n",
        "  return np.array(distances).mean()\r\n",
        "\r\n",
        "def calculate_levenstein_test(indices, target_indices):\r\n",
        "  distances = []\r\n",
        "  indices = indices.detach().cpu().numpy()\r\n",
        "  target_indices = target_indices.cpu().numpy()\r\n",
        "  for i in range(len(target_indices)):\r\n",
        "    distances.append(Levenshtein.distance(convert_to_string(config.indices_dict, target_indices[i])[6:], \r\n",
        "                                          convert_to_string(config.indices_dict, indices[i])[6:]))\r\n",
        "  return np.array(distances).mean()\r\n",
        "\r\n",
        "def predict(test_df, model, test_loader, indices_dict):\r\n",
        "    test_df = test_df.copy()\r\n",
        "    InChI = []\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      for batch in tqdm(test_loader):\r\n",
        "        images = batch[\"images\"].cuda()\r\n",
        "        logits = model.generate(images)\r\n",
        "        pred_indices = torch.argmax(logits, dim = -1)\r\n",
        "        for row in pred_indices:\r\n",
        "          InChI.append(convert_to_string(indices_dict, row))\r\n",
        "    test_df['InChI'] = InChI\r\n",
        "    return test_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTrYpKtpIDZ"
      },
      "source": [
        "class CustomRunner(dl.Runner):\r\n",
        "\r\n",
        "    def _handle_batch(self, batch):\r\n",
        "        images = batch[\"images\"]\r\n",
        "        indices = batch[\"indices\"]\r\n",
        "\r\n",
        "        if self.loader_key == 'valid':\r\n",
        "          self.model.eval()\r\n",
        "          output = self.model.generate(images)\r\n",
        "          levenstein = calculate_levenstein_test(output, indices)\r\n",
        "          loss = 0\r\n",
        "          f1 = 0\r\n",
        "        else:\r\n",
        "          self.model.train()\r\n",
        "          logits = self.model(indices[:, :-1], images)\r\n",
        "          output = logits.reshape(-1, logits.shape[-1])\r\n",
        "          trg = indices[:, 1:].reshape(-1)\r\n",
        "\r\n",
        "          loss = criterion(output, trg)\r\n",
        "          f1 = calculate_f1(output, trg, torch.where(trg)[0])\r\n",
        "          levenstein = calculate_levenstein(logits, indices, config.indices_dict)\r\n",
        "\r\n",
        "        batch_metrics = {\r\n",
        "              \"loss\": loss,\r\n",
        "              \"f1_score\": f1,\r\n",
        "              \"levenstein\": levenstein\r\n",
        "              }\r\n",
        "        self.batch_metrics.update(batch_metrics)\r\n",
        "\r\n",
        "        if self.is_train_loader:\r\n",
        "          loss.backward()\r\n",
        "          # nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\r\n",
        "          self.optimizer.step()\r\n",
        "          self.optimizer.zero_grad()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u05fmTEwvOp"
      },
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\r\n",
        "\r\n",
        "EPOCHS = 3\r\n",
        "transform = Compose([\r\n",
        "    ToPILImage('RGB'),\r\n",
        "    Resize((256,256)),\r\n",
        "    ToTensor(),\r\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "    ])\r\n",
        "\r\n",
        "transform_test = Compose([\r\n",
        "    ToPILImage('RGB'),\r\n",
        "    Resize((256,256)),\r\n",
        "    ToTensor(),\r\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "    ])\r\n",
        "\r\n",
        "train_dataset = Dataset(df_train, 'train', char_dict, transform)\r\n",
        "val_dataset = Dataset(df_val, 'train', char_dict, transform_test)\r\n",
        "# test_dataset = Dataset(df_test, 'train', char_dict, transform)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, config.batch_size, num_workers = 4, collate_fn=collate_fn, shuffle=True)\r\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, config.batch_size, num_workers = 4, collate_fn = collate_fn)\r\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size, collate_fn=collate_fn)\r\n",
        "\r\n",
        "# encoder = CNN(config.image_embedding)\r\n",
        "# decoder = RNN(config)\r\n",
        "model = Img2Text(config)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay = 1e-4)\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\r\n",
        "scheduler = OneCycleLR(optimizer, 1e-3, total_steps=len(train_loader)*EPOCHS)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKA0auPdaGWS"
      },
      "source": [
        "# checkpoint = torch.load(\"/content/drive/MyDrive/image2text/logs/checkpoints/last_full.pth\")\r\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\r\n",
        "# criterion.load_state_dict(checkpoint['criterion_state_dict'])\r\n",
        "# model.train()\r\n",
        "# EPOCH = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xnohDl1frD"
      },
      "source": [
        "runner = CustomRunner()\r\n",
        "runner.train(\r\n",
        "    model=model,\r\n",
        "    criterion= criterion,\r\n",
        "    optimizer=optimizer,\r\n",
        "    scheduler=scheduler,\r\n",
        "    loaders={\r\n",
        "        'train': train_loader,\r\n",
        "        'valid': val_loader,\r\n",
        "           },\r\n",
        "    logdir=\"/content/drive/MyDrive/image2text/logs\",\r\n",
        "    num_epochs=1,\r\n",
        "    verbose=True,\r\n",
        "    load_best_on_end=True,\r\n",
        "    overfit=False,\r\n",
        "    callbacks=[SchedulerCallback(reduced_metric = 'loss', mode = 'batch')],\r\n",
        "    main_metric=\"levenstein\",\r\n",
        "    minimize_metric=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0ITI3LBVtc"
      },
      "source": [
        "tqdm._instances.clear()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMrSMm4aWRT"
      },
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/logs/checkpoints/best.pth')['model_state_dict'])\r\n",
        "model.cuda()\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghrRzCofvXR"
      },
      "source": [
        "pd_df = predict(df_test, model, test_loader, config.indices_dict)\r\n",
        "pd_df['InChI_true'] = df_test['InChI']\r\n",
        "pd_df = pd_df.assign(levenstein = lambda x: calculate_levenstein_test(x.InChI_true.values, x.InChI.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l24k_1i_L0DM"
      },
      "source": [
        "pd_df.levenstein.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkaHRQKMcLG",
        "outputId": "3fd1e9d0-99e2-4876-b9f5-bc7aea58e5bc"
      },
      "source": [
        "test_dataset = TestDataset(submission, transform_test)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size)\r\n",
        "pd_df = predict(submission, model, test_loader, config.indices_dict)\r\n",
        "pd_df.to_csv('/content/drive/MyDrive/image2text/submission.csv', index = False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12626/12626 [2:32:43<00:00,  1.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3olisffumd1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}