{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image2text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a06d78d552f14687b8cbcf2ef3c73850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_83cc3993a4ef471fa29242765ce43f6e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba5729ae7afd4f069ebe5b124b93af96",
              "IPY_MODEL_0ea7ce2c44d34551bc46ad97cd216521"
            ]
          }
        },
        "83cc3993a4ef471fa29242765ce43f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba5729ae7afd4f069ebe5b124b93af96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0d64fb516a5a4f65a290f89fae79fda5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8981c25793c845f6b4c6a90b594a9320"
          }
        },
        "0ea7ce2c44d34551bc46ad97cd216521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14abdbf7a1ae491fa62ba8b73829d1af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:06&lt;00:00, 14.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9eaff9d429641169aee5f360ced1830"
          }
        },
        "0d64fb516a5a4f65a290f89fae79fda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8981c25793c845f6b4c6a90b594a9320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14abdbf7a1ae491fa62ba8b73829d1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9eaff9d429641169aee5f360ced1830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4_xv70ow1g"
      },
      "source": [
        "!pip install catalyst==20.12 python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0d7nGG2e8XN"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomVerticalFlip, RandomHorizontalFlip, ToPILImage\r\n",
        "import torchvision\r\n",
        "import catalyst\r\n",
        "import random\r\n",
        "from catalyst import dl, utils\r\n",
        "from catalyst.callbacks.scheduler import SchedulerCallback\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score, f1_score\r\n",
        "import Levenshtein\r\n",
        "import cv2\r\n",
        "from PIL import Image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7itVi2LmIVKE"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDB2svjETFPv"
      },
      "source": [
        "!unzip /content/drive/MyDrive/image2text/train.zip -d /content/train/\r\n",
        "!unzip /content/drive/MyDrive/image2text/train_labels.csv.zip\r\n",
        "\r\n",
        "!unzip /content/drive/MyDrive/image2text/test.zip -d /content/test/\r\n",
        "!cp /content/drive/MyDrive/image2text/sample_submission.csv /content/sample_submission.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYG0CGqe8bC"
      },
      "source": [
        "TRAIN_SIZE = 0.9\r\n",
        "TRAIN_PATH = '/content/train_labels.csv'\r\n",
        "TEST_PATH = '/content/sample_submission.csv'\r\n",
        "TRAIN_IMAGES = '/content/train'\r\n",
        "TEST_IMAGES = '/content/test'\r\n",
        "submission = pd.read_csv(TEST_PATH)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o68OvFsA0Fc"
      },
      "source": [
        "df = pd.read_csv(TRAIN_PATH)\r\n",
        "df['len'] = df['InChI'].apply(len)\r\n",
        "df['bucket'] = pd.qcut(df['len'], 10)\r\n",
        "df_train, df_val = train_test_split(df, train_size = TRAIN_SIZE, random_state = 42, stratify = df['bucket'])\r\n",
        "# df_val, df_test = train_test_split(df_val_test, train_size = 0.33, stratify = df_val_test['bucket'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uI35wz4e8dj",
        "outputId": "bbb2dd6a-7f9e-402a-bf56-eb360b3da721"
      },
      "source": [
        "char_dict = {'<PAD>': 0,\r\n",
        "             'InChI=1S/': 1,\r\n",
        "             '<UNK>': 2, \r\n",
        "             '<EOS>': 3}\r\n",
        "for _, row in tqdm(df_train.iterrows()):\r\n",
        "  for char in row['InChI']:\r\n",
        "    if char not in char_dict:\r\n",
        "      char_dict[char] = len(char_dict)\r\n",
        "\r\n",
        "indices_dict = dict(map(lambda x: (x[1], x[0]), char_dict.items()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2181767it [02:48, 12972.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM1E1vfsJq8n"
      },
      "source": [
        "class Config:\r\n",
        "  max_len = 250\r\n",
        "  batch_size = 128\r\n",
        "  hidden_size = 300\r\n",
        "  cell_size = 300\r\n",
        "  num_layers = 2\r\n",
        "  emb_size = 200\r\n",
        "  attention_dim = 300\r\n",
        "  teacher_forcing = 1\r\n",
        "  dropout_1 = 0.3\r\n",
        "  dropout_2 = 0.2\r\n",
        "  image_embedding = 512\r\n",
        "  char_dict = char_dict\r\n",
        "  indices_dict = indices_dict\r\n",
        "  vocab_size = len(char_dict)\r\n",
        "\r\n",
        "config = Config"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kBqa1N-e80W"
      },
      "source": [
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        \r\n",
        "        self.attention_dim = attention_dim\r\n",
        "        \r\n",
        "        self.W = nn.Linear(decoder_dim,attention_dim)\r\n",
        "        self.U = nn.Linear(encoder_dim,attention_dim)\r\n",
        "        \r\n",
        "        self.A = nn.Linear(attention_dim,1)\r\n",
        "          \r\n",
        "    def forward(self, features, hidden_state):\r\n",
        "        hidden_state = hidden_state.mean(0)\r\n",
        "        u_hs = self.U(features)     #(batch_size,64,attention_dim)\r\n",
        "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\r\n",
        "        \r\n",
        "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,64,attemtion_dim)\r\n",
        "        \r\n",
        "        attention_scores = self.A(combined_states)         #(batch_size,64,1)\r\n",
        "        attention_scores = attention_scores.squeeze(2)     #(batch_size,64)\r\n",
        "        \r\n",
        "        \r\n",
        "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,64)\r\n",
        "        \r\n",
        "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,64,features_dim)\r\n",
        "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,64)\r\n",
        "        \r\n",
        "        return alpha,attention_weights\r\n",
        "\r\n",
        "\r\n",
        "class CNN(nn.Module):\r\n",
        "  def __init__(self, emb_size = 512):\r\n",
        "    super().__init__()\r\n",
        "    self.emb_size = emb_size\r\n",
        "    resnet = torchvision.models.resnet34(pretrained=True)\r\n",
        "    modules = list(resnet.children())[:-2]\r\n",
        "    self.resnet = nn.Sequential(*modules)\r\n",
        "\r\n",
        "    # self.conv1 = Conv2D(3, 6, 256)\r\n",
        "    # self.conv2 = Conv2D(6, 12, 128)\r\n",
        "    # self.conv3 = Conv2D(12, 24, 64)\r\n",
        "    # self.conv4 = Conv2D(24, 24, 32)\r\n",
        "    # self.fc = nn.Sequential(\r\n",
        "    #     nn.Linear(32768, emb_size),\r\n",
        "    # )\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.resnet(x)\r\n",
        "    # x = x.reshape(x.shape[0], -1)\r\n",
        "    # x = self.fc(x)\r\n",
        "    x = x.permute(0, 2, 3, 1)                           #(batch_size,8,8,512)\r\n",
        "    x = x.view(x.size(0), -1, x.size(-1))\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "class RNN(nn.Module):\r\n",
        "  def __init__(self, config):\r\n",
        "    super().__init__()\r\n",
        "    self.hidden_size = config.hidden_size\r\n",
        "    self.embedding = nn.Embedding(config.vocab_size, config.emb_size)\r\n",
        "    self.dropout_emb = nn.Dropout(config.dropout_1)\r\n",
        "    self.dropout_hidden = nn.Dropout(config.dropout_2)\r\n",
        "    self.rnn = nn.LSTM(input_size=config.emb_size + config.image_embedding,\r\n",
        "                       hidden_size=config.hidden_size,\r\n",
        "                       num_layers = config.num_layers,\r\n",
        "                       batch_first = True)\r\n",
        "    self.attention = Attention(config.image_embedding, config.hidden_size, config.attention_dim)\r\n",
        "    self.fc_score = nn.Linear(config.hidden_size, config.vocab_size)\r\n",
        "\r\n",
        "  def forward(self, indices, features, hidden, cell, teacher_forcing_ratio = 0.5):\r\n",
        "    #input - индексы\r\n",
        "    input = self.embedding(indices)\r\n",
        "    outputs = torch.zeros(input.shape[0], config.max_len, config.vocab_size, requires_grad=True).cuda()\r\n",
        "    input = input[:, 0] #(batch_size, emb_size)\r\n",
        "\r\n",
        "    for t in range(1, config.max_len-1):\r\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\r\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\r\n",
        "\r\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\r\n",
        "      hidden = self.dropout_hidden(hidden)\r\n",
        "\r\n",
        "      output = self.fc_score(output.squeeze())\r\n",
        "      outputs[:, t] = output\r\n",
        "\r\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "      top1 = output.max(1)[1]\r\n",
        "      input = (indices[:, t] if teacher_force else top1)\r\n",
        "      input = self.dropout_emb(self.embedding(input))\r\n",
        "\r\n",
        "    return outputs\r\n",
        "\r\n",
        "  def generate(self, features, hidden, cell):\r\n",
        "\r\n",
        "    outputs = torch.zeros(features.shape[0], config.max_len, config.vocab_size, requires_grad=True).cuda()\r\n",
        "    input = config.char_dict['InChI=1S/']*torch.ones(features.shape[0]).long().cuda()\r\n",
        "    input = self.embedding(input)\r\n",
        "\r\n",
        "    for t in range(1, config.max_len-1):\r\n",
        "      alpha, attn = self.attention(features, hidden) #attn = (batch_size, 64)\r\n",
        "      input, attn = input.unsqueeze(1), attn.unsqueeze(1)\r\n",
        "\r\n",
        "      output, (hidden, cell) = self.rnn(torch.cat([input, attn], dim = -1), (hidden, cell))\r\n",
        "      hidden = self.dropout_hidden(hidden)\r\n",
        "\r\n",
        "      output = self.fc_score(output.squeeze())\r\n",
        "      outputs[:, t] = output\r\n",
        "\r\n",
        "      input = output.max(1)[1]\r\n",
        "      input = self.dropout_emb(self.embedding(input))\r\n",
        "\r\n",
        "    return outputs\r\n",
        "\r\n",
        "\r\n",
        "class Img2Text(nn.Module):\r\n",
        "  def __init__(self, encoder, decoder, config):\r\n",
        "    super().__init__()\r\n",
        "    self.config = config\r\n",
        "    self.fc_h0 = nn.Linear(encoder.emb_size, decoder.hidden_size)\r\n",
        "    self.fc_c0 = nn.Linear(encoder.emb_size, decoder.hidden_size)\r\n",
        "\r\n",
        "    self.encoder = encoder\r\n",
        "    self.decoder = decoder\r\n",
        "\r\n",
        "  def forward(self, image, text, teacher_forcing_ratio = 0.5):\r\n",
        "    img_vector = self.encoder(image) #(batch, 64, 512)\r\n",
        "\r\n",
        "    hidden = self.fc_h0(img_vector.mean(1).squeeze())\r\n",
        "    cell = self.fc_c0(img_vector.mean(1).squeeze())\r\n",
        "    hidden = hidden.reshape(1, hidden.shape[0], -1)\r\n",
        "    hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\r\n",
        "    cell = cell.reshape(1, cell.shape[0], -1)\r\n",
        "    cell = cell.repeat(self.config.num_layers, 1, 1)\r\n",
        "\r\n",
        "    outputs = self.decoder(text, img_vector, hidden, cell, teacher_forcing_ratio)\r\n",
        "    return outputs\r\n",
        "  \r\n",
        "  def generate(self, image):\r\n",
        "    self.encoder.eval()\r\n",
        "    self.decoder.eval()\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      img_vector = self.encoder(image)\r\n",
        "\r\n",
        "      hidden = self.fc_h0(img_vector.mean(1).squeeze())\r\n",
        "      cell = self.fc_c0(img_vector.mean(1).squeeze())\r\n",
        "      hidden = hidden.reshape(1, hidden.shape[0], -1)\r\n",
        "      hidden = hidden.repeat(self.config.num_layers, 1, 1) #(num_layers, batch, hidden)\r\n",
        "      cell = cell.reshape(1, cell.shape[0], -1)\r\n",
        "      cell = cell.repeat(self.config.num_layers, 1, 1)\r\n",
        "\r\n",
        "      outputs = self.decoder.generate(img_vector, hidden, cell)\r\n",
        "    return outputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl7g3rJGzQBf"
      },
      "source": [
        "def convert_to_indices(char_dict, string):\r\n",
        "  indices = [char_dict['InChI=1S/']]\r\n",
        "  for char in string[6:]:\r\n",
        "    if char not in char_dict:\r\n",
        "      indices_append(char_dict['<UNK>'])\r\n",
        "    else:\r\n",
        "      indices.append(char_dict[char])\r\n",
        "  indices += [char_dict['<EOS>']]\r\n",
        "  return indices\r\n",
        "\r\n",
        "def convert_to_string(indices_dict, indices):\r\n",
        "  string = 'InChI='\r\n",
        "  if isinstance(indices, torch.Tensor):\r\n",
        "    indices = indices.detach().cpu().numpy()\r\n",
        "  for indx in indices[1:]:\r\n",
        "    if indices_dict[indx] == '<EOS>' or indices_dict[indx] == '<PAD>':\r\n",
        "      break\r\n",
        "    else:\r\n",
        "      string += indices_dict[indx]\r\n",
        "  return string\r\n",
        "\r\n",
        "def get_path(mode, image_id):\r\n",
        "    if mode == 'train':\r\n",
        "      return f'{TRAIN_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\r\n",
        "    elif mode == 'test':\r\n",
        "      return f'{TEST_IMAGES}/{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.png'\r\n",
        "    else:\r\n",
        "      raise NameError\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, df, mode, char_dict, transforms):\r\n",
        "    super().__init__()\r\n",
        "    self.df = df\r\n",
        "    self.mode = mode\r\n",
        "    self.char_dict = char_dict\r\n",
        "    self.transform = transforms\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.df)\r\n",
        "\r\n",
        "  def __getitem__(self, indx):\r\n",
        "    row = self.df.iloc[indx]\r\n",
        "    image_path = get_path(self.mode, row['image_id'])\r\n",
        "    indices = torch.tensor(convert_to_indices(self.char_dict, row['InChI']))\r\n",
        "    return {'images': self.get_image(image_path),\r\n",
        "            'indices': indices}\r\n",
        "\r\n",
        "  def get_image(self, image_path):\r\n",
        "    img = cv2.imread(image_path)\r\n",
        "    img = self.transform(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "class TestDataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, df, transforms):\r\n",
        "    super().__init__()\r\n",
        "    self.df = df\r\n",
        "    self.transform = transforms\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.df)\r\n",
        "\r\n",
        "  def __getitem__(self, indx):\r\n",
        "    row = self.df.iloc[indx]\r\n",
        "    image_path = get_path('test', row['image_id'])\r\n",
        "    return {'images': self.get_image(image_path)}\r\n",
        "\r\n",
        "  def get_image(self, image_path):\r\n",
        "    transform = A.Compose([A.Transpose(p=1),\r\n",
        "                           A.VerticalFlip(p=1)\r\n",
        "                          ])\r\n",
        "    \r\n",
        "    img = cv2.imread(image_path)\r\n",
        "    h, w, _ = img.shape\r\n",
        "    if h > w:\r\n",
        "      img = transform(image=img)['image']\r\n",
        "    img = self.transform(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "def collate_fn(batch):\r\n",
        "  images = torch.cat([item['images'].unsqueeze(0) for item in batch], dim = 0)\r\n",
        "  indices = torch.zeros(len(batch), config.max_len).long()\r\n",
        "  for i, item in enumerate(batch):\r\n",
        "    ind = item['indices'][:config.max_len]\r\n",
        "    indices[i][:len(ind)] = ind\r\n",
        "\r\n",
        "  return {'images': images,\r\n",
        "          'indices': indices}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgcgxUju5S9b"
      },
      "source": [
        "def calculate_accuracy(logits, targets, mask):\r\n",
        "    mask = mask.detach().cpu().numpy()\r\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\r\n",
        "    targets = targets.detach().cpu().numpy()\r\n",
        "    return accuracy_score(targets[mask], logits[mask])\r\n",
        "\r\n",
        "def calculate_f1(logits, targets, mask):\r\n",
        "    mask = mask.detach().cpu().numpy()\r\n",
        "    logits = torch.argmax(logits, dim = -1).detach().cpu().numpy()\r\n",
        "    targets = targets.detach().cpu().numpy()\r\n",
        "    return f1_score(targets[mask], logits[mask], average = 'micro')\r\n",
        "\r\n",
        "def calculate_levenstein(logits, targets, indices_dict):\r\n",
        "  distances = []\r\n",
        "  logits = torch.argmax(logits, dim = -1)\r\n",
        "  targets = targets.detach()\r\n",
        "  for indx in range(len(targets)):\r\n",
        "    distances.append(Levenshtein.distance(convert_to_string(indices_dict, targets[indx]), \r\n",
        "                                          convert_to_string(indices_dict, logits[indx])))\r\n",
        "  return np.array(distances).mean()\r\n",
        "\r\n",
        "def calculate_levenstein_test(true_string, target_string):\r\n",
        "  distances = []\r\n",
        "  for indx in range(len(target_string)):\r\n",
        "    distances.append(Levenshtein.distance(true_string[indx][6:], \r\n",
        "                                          target_string[indx][6:]))\r\n",
        "  return np.array(distances)\r\n",
        "\r\n",
        "def predict(test_df, model, test_loader, indices_dict):\r\n",
        "    test_df = test_df.copy()\r\n",
        "    InChI = []\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "      for batch in tqdm(test_loader):\r\n",
        "        images = batch[\"images\"].cuda()\r\n",
        "        logits = model.generate(images)\r\n",
        "        pred_indices = torch.argmax(logits, dim = -1)\r\n",
        "        for row in pred_indices:\r\n",
        "          InChI.append(convert_to_string(indices_dict, row))\r\n",
        "    test_df['InChI'] = InChI\r\n",
        "    return test_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeTrYpKtpIDZ"
      },
      "source": [
        "class CustomRunner(dl.Runner):\r\n",
        "\r\n",
        "    def _handle_batch(self, batch):\r\n",
        "        images = batch[\"images\"]\r\n",
        "        indices = batch[\"indices\"]\r\n",
        "\r\n",
        "        if self.loader_key == 'valid':\r\n",
        "          self.model.eval()\r\n",
        "          logits = self.model(images, indices[:, :-1], 0)\r\n",
        "        else:\r\n",
        "          self.model.train()\r\n",
        "          logits = self.model(images, indices[:, :-1], config.teacher_forcing)\r\n",
        "        output = logits[:, 1:].reshape(-1, logits.shape[-1])\r\n",
        "        trg = indices[:, 1:].reshape(-1)\r\n",
        "          \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        levenstein = calculate_levenstein(logits, indices, config.indices_dict)\r\n",
        "\r\n",
        "        batch_metrics = {\r\n",
        "              \"loss\": loss,\r\n",
        "              \"f1_score\": calculate_f1(output, trg, torch.where(trg)[0]),\r\n",
        "              \"levenstein\": levenstein\r\n",
        "              }\r\n",
        "        self.batch_metrics.update(batch_metrics)\r\n",
        "\r\n",
        "        if self.is_train_loader:\r\n",
        "          loss.backward()\r\n",
        "          # nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\r\n",
        "          self.optimizer.step()\r\n",
        "          self.optimizer.zero_grad()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u05fmTEwvOp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "a06d78d552f14687b8cbcf2ef3c73850",
            "83cc3993a4ef471fa29242765ce43f6e",
            "ba5729ae7afd4f069ebe5b124b93af96",
            "0ea7ce2c44d34551bc46ad97cd216521",
            "0d64fb516a5a4f65a290f89fae79fda5",
            "8981c25793c845f6b4c6a90b594a9320",
            "14abdbf7a1ae491fa62ba8b73829d1af",
            "d9eaff9d429641169aee5f360ced1830"
          ]
        },
        "outputId": "3385c7f5-b8ed-4dd0-8813-ad4c0cce47ee"
      },
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\r\n",
        "\r\n",
        "EPOCHS = 3\r\n",
        "transform = Compose([\r\n",
        "    ToPILImage('RGB'),\r\n",
        "    Resize((256,256)),\r\n",
        "    RandomVerticalFlip(p = 0.5),\r\n",
        "    ToTensor(),\r\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "    ])\r\n",
        "\r\n",
        "transform_test = Compose([\r\n",
        "    ToPILImage('RGB'),\r\n",
        "    Resize((256,256)),\r\n",
        "    ToTensor(),\r\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "    ])\r\n",
        "\r\n",
        "train_dataset = Dataset(df_train, 'train', char_dict, transform)\r\n",
        "val_dataset = Dataset(df_val, 'train', char_dict, transform_test)\r\n",
        "# test_dataset = Dataset(df_test, 'train', char_dict, transform)\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, config.batch_size, num_workers = 4, collate_fn=collate_fn, shuffle=True)\r\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, config.batch_size, num_workers = 4, collate_fn = collate_fn)\r\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size, collate_fn=collate_fn)\r\n",
        "\r\n",
        "encoder = CNN(config.image_embedding)\r\n",
        "decoder = RNN(config)\r\n",
        "model = Img2Text(encoder, decoder, config)\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), weight_decay = 1e-4)\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = 0)\r\n",
        "scheduler = OneCycleLR(optimizer, 1e-3, total_steps=len(train_loader)*EPOCHS)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06d78d552f14687b8cbcf2ef3c73850",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xnohDl1frD"
      },
      "source": [
        "runner = CustomRunner()\r\n",
        "runner.train(\r\n",
        "    model=model,\r\n",
        "    criterion= criterion,\r\n",
        "    optimizer=optimizer,\r\n",
        "    scheduler=scheduler,\r\n",
        "    loaders={'train': train_loader,\r\n",
        "             'valid': val_loader\r\n",
        "             },\r\n",
        "    logdir=\"/content/drive/MyDrive/image2text/logs\",\r\n",
        "    num_epochs=EPOCHS,\r\n",
        "    verbose=True,\r\n",
        "    load_best_on_end=True,\r\n",
        "    overfit=False,\r\n",
        "    callbacks=[SchedulerCallback(reduced_metric = 'loss', mode = 'batch')],\r\n",
        "    main_metric=\"levenstein\",\r\n",
        "    minimize_metric=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc0ITI3LBVtc"
      },
      "source": [
        "tqdm._instances.clear()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSMrSMm4aWRT"
      },
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/image2text/logs/checkpoints/best.pth')['model_state_dict'])\r\n",
        "model.cuda()\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghrRzCofvXR"
      },
      "source": [
        "pd_df = predict(df_test, model, test_loader, config.indices_dict)\r\n",
        "pd_df['InChI_true'] = df_test['InChI']\r\n",
        "pd_df = pd_df.assign(levenstein = lambda x: calculate_levenstein_test(x.InChI_true.values, x.InChI.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l24k_1i_L0DM"
      },
      "source": [
        "pd_df.levenstein.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkaHRQKMcLG",
        "outputId": "3fd1e9d0-99e2-4876-b9f5-bc7aea58e5bc"
      },
      "source": [
        "test_dataset = TestDataset(submission, transform_test)\r\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, config.batch_size)\r\n",
        "pd_df = predict(submission, model, test_loader, config.indices_dict)\r\n",
        "pd_df.to_csv('/content/drive/MyDrive/image2text/submission.csv', index = False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12626/12626 [2:32:43<00:00,  1.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3olisffumd1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}